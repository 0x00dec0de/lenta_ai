{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация заголовков новостей с использованием Char-RNN\n",
    "Идея Char-RNN описана в [статье Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "Первоначальная версия кода взята из [примера реализации Char-RNN на Keras](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь задаем, на каких данных будем обучать сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Все статьи\n",
    "# mode = \"full\"       \n",
    "\n",
    "# Выборка (~800 статей)\n",
    "mode = \"sample\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем файл с предварительно обработанным текстом (см. ноутбук [preprocessing](/preprocessing.ipynb)) и оценим количество слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('./data/headers_' + mode + '.txt', 'r', encoding='utf-8')\n",
    "text = f.read().lower()\n",
    "print('Размер корпуса:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем набор всех символов в тексте и составляем 2 словаря:\n",
    "* `char_indices` содержит символы и соотвествующий им индекс\n",
    "* `indices_char` позволит по индексу получить символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Всего символов:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательная функция, выдающая случайный символ из наиболее вероятных. `temperature` отвечает за вариативность выдаваемых индексов: более низкое значение будет выдавать более вероятный символ с меньшей вариативностью, и наоборот.\n",
    "\n",
    "На вход функции подаются вероятности, предсказанные сетью, на выходе выдается индекс символа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разрезаем текст на куски по `maxlen` символов с шагом `step`. Чем больше `maxlen`, тем более \"глубокими\" становятся зависимости последующих символов от предыдущих. Также, чем меньше `step`, тем больше различных комбинаций символов попадет в обучающие данные. \n",
    "\n",
    "При `maxlen` = 5 и `step` = 2 фраза \"Ну что, как дела?\" будет разрезана на:\n",
    "* \"Ну чт\"\n",
    "* \" что \"\n",
    "* \"то, ка\"\n",
    "* \", как \"\n",
    "и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Последовательностей:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные последовательности символов в последовательности векторов, в которых каждому символу будет соотвествовать вектор [x1, x2, ..., xK, ... xN], где xK = 1, если K равно индексу данного символа, N = количество в нашем наборе `chars`, а все остальные значения равны нулю. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Задаем гиперпараметры:\n",
    "* `epochs`: количество эпох (итераций) обучения;\n",
    "* `batch_size`: размер мини-батча, чем он меньше, тем менее усредненной будет ошибка, соответственно, будет выше точность и меньше вероятность скатиться в локальный минимум;\n",
    "* `options`: массив опций для обучения; используется для того, чтобы определить наиболее удачную конфигурацию нейронной сети; для обучения только по одному набору параметров, можно закомментировать остальные наборы;\n",
    "* `rnn_size`: количество LSTM-ячеек;\n",
    "* `rnn_layers`: количество LSTM-слоев;\n",
    "* `gen_length`: длина генерируемого текста в символах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "options = [\n",
    "#     {\n",
    "#         'rnn_size': 32,\n",
    "#         'rnn_layers': 1\n",
    "#     },\n",
    "#     {\n",
    "#         'rnn_size': 64,\n",
    "#         'rnn_layers': 1\n",
    "#     },\n",
    "#     {\n",
    "#         'rnn_size': 32,\n",
    "#         'rnn_layers': 2\n",
    "#     },\n",
    "    {\n",
    "        'rnn_size': 64,\n",
    "        'rnn_layers': 2\n",
    "    }\n",
    "]\n",
    "gen_length = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Обучение\n",
    "Для каждого набора параметров строим модель, обучаем ее за `epochs` итераций, после каждой итерации выводим сгенерированный текст, а также сохраняем его в файл. Кроме того, на каждой итерации сохраняем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for option in options:\n",
    "    \n",
    "    # Обнуляем сессию\n",
    "    K.clear_session()\n",
    "    print('Тренируем модель Char-RNN с параметрами (слоев: {}, размер: {})'.format(option['rnn_layers'], option['rnn_size']))\n",
    "    \n",
    "    # Сохраняем время начала обучения для этого набора параметров\n",
    "    option['start'] = time.time()\n",
    "    \n",
    "    # Строим модель\n",
    "    model = Sequential()\n",
    "    if option['rnn_layers'] == 1:\n",
    "        model.add(LSTM(option['rnn_layers'], input_shape=(maxlen, len(chars))))\n",
    "    else:\n",
    "        model.add(LSTM(option['rnn_layers'], input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "        model.add(LSTM(option['rnn_layers']))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.01, decay=1e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # Результаты будут сохраняться в этот файл\n",
    "    with open('results_char_rnn_{}_{}.txt'.format(option['rnn_layers'], option['rnn_size']), 'a') as file:\n",
    "        \n",
    "        # Цикл с эпохами\n",
    "        for iteration in range(1, epochs):\n",
    "            print()\n",
    "            print('-' * 50)\n",
    "            print('Эпоха ', iteration)\n",
    "\n",
    "            # Здесь ставим 1 эпоху, т.к. за итерации отвечает верхний цикл\n",
    "            # Это сделано для того, чтобы после каждой итерации можно было провести дополнительные операции,\n",
    "            # а возиться с callback'ами не очень хотелось...\n",
    "            h = model.fit(x, y,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=1,\n",
    "                      verbose=2)\n",
    "\n",
    "            # Случайно выбираем начало входной последовательности\n",
    "            start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "            \n",
    "            file.write('Эпоха ' + str(iteration) + '\\n')\n",
    "            file.write(str(h.history) + '\\n')\n",
    "            \n",
    "            # Выводим результаты генерации с разными температурами (вариативностью)\n",
    "            for diversity in [0.5, 1.0]:\n",
    "                print()\n",
    "                print('----- diversity:', diversity)\n",
    "                file.write('----- diversity: ' +str(diversity) + '\\n')\n",
    "                generated = ''\n",
    "                sentence = text[start_index: start_index + maxlen]\n",
    "                generated += sentence\n",
    "                print('----- Начальная последовательность: \"' + sentence + '\"')\n",
    "                file.write('----- Начальная последовательность: \"' + sentence + '\"\\n')\n",
    "                file.write(generated)\n",
    "                \n",
    "                # Запускаем посимвольную генерацию\n",
    "                for i in range(gen_length):\n",
    "                    # Создаем массив нулей, в который запишем векторы символов входной строки\n",
    "                    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x_pred[0, t, char_indices[char]] = 1.\n",
    "                    next_char = \"\"\n",
    "                    \n",
    "                    # На всякий случай делаем это в цикле, т.к. бывали случаи, когда генерируемый символ почему-то\n",
    "                    # оказывался не из нашего набора (какой-то баг), в таком случае генерируем другой символ\n",
    "                    while next_char not in chars:\n",
    "                        preds = model.predict(x_pred, verbose=0)[0]\n",
    "                        next_index = sample(preds, diversity)\n",
    "                        next_char = indices_char[next_index]\n",
    "\n",
    "                    # Добавляем новый символ к строке результата\n",
    "                    generated += next_char\n",
    "                    sentence = sentence[1:] + next_char\n",
    "                \n",
    "                print(generated + '\\n')\n",
    "                file.write(next_char)\n",
    "                file.write('\\n')\n",
    "            \n",
    "            file.write('------------------------------------------\\n')\n",
    "\n",
    "            # Тут считаем время, оставшееся до завершения обучения по текущему набору параметров\n",
    "            t = time.time()\n",
    "            time_diff = t - option['start']\n",
    "            total_time = round((epochs / (iteration + 1)) * time_diff)\n",
    "            rem_time = round(total_time - time_diff)\n",
    "            m, s = divmod(rem_time, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "            print(\"Оценка оставшегося времени на обучение по текущему набору параметров: %d:%02d:%02d \" % (h, m, s), end=\"\\r\")\n",
    "\n",
    "            # И сохраняем модель\n",
    "            model.save(\"./models/char_rnn_{}_{}.h5\".format(option['rnn_layers'], option['rnn_size']))\n",
    "\n",
    "        # В конце считаем время, затраченное на обучение по текущему набору параметров\n",
    "        time_diff = time.time() - option['start']\n",
    "        m, s = divmod(time_diff, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "\n",
    "        print(\"Затраченное время: %d:%02d:%02d \" % (h, m, s))\n",
    "        file.write(\"Затраченное время: %d:%02d:%02d \" % (h, m, s))\n",
    "        print('Обучение модели Char-RNN с параметрами {} {} завершено'.format(option['rnn_layers'], option['rnn_size']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
